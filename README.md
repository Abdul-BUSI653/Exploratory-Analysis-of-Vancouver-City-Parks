# Exploratory-Analysis-of-Vancouver-City-Parks
A project which explain on the DAP design and implementation for the exploratory analysis of the "Parks" segment of city of Vancouver.

* The dataset taken by me is about the  “Parks” segment of the “Parts, Recreation and Pets” module of the Vancouver city portal.
* In this we have key features like the ‘Park Name’, ‘Special Features’, ‘Facilities’, ‘Washroom’,  ‘Address’ and other details.
* From this, the main objective the teammate has taken to do analysis is “What is the count of Official parks in each Neighbourhood?” to further analyse and find any trends available
#### Data Questions (Metric)
First we need to gather a few details below:
* Total count of parks in each neighborhood
•	Total count of Official parks in each neighborhood
•	Find the ratio of the official parks to the total count of parks in each neighbourhood respectively.<br>
Below I mentioned the 4 step process involved in the DAP design related to the exploratory question posted above analysis (highlighted and underlined).<br>
We are going to use the AWS services like S3, Glue, Glue DataBrew as per our need to store the results and other details of the project.
#### Dap Design
![image 000](https://github.com/user-attachments/assets/4121d110-4bfd-4b89-bfa0-93d88a47e082)<br>
The above image displays the DAP design we are implementing for the descyptive analysis.
#### Descriptive  Question for Analysis
![image 002](https://github.com/user-attachments/assets/b9db6e00-d7c1-453e-bf1f-67058be627b7)<br>
The above images displays the exploratory analysis of our DAP model.The above figure displays the analysis shown based on the conditions set in the Vancouver portal for the ‘Official’ columns and the ‘Neighbourhood’ columns.
#### Step 1: Data Ingestion
* This step explains about the Data ingestion into AWS Environment.
* We have downloaded the dataset as an excel file which we will be moving to the ‘S3’ buckets of AWS.
* Prior to this we created a bucket called “vprp-raw-abdul” in that he created a folder called ‘Ingestion_year-2024’ to store the information of year 2024.
* Now we have the S3 bucket and folder to ingest our dataset into it. I then uploaded the dataset into the folder.
![image 003](https://github.com/user-attachments/assets/06cca1f5-7388-47a3-8f3a-b3aaefa58cf2)<br>
* Above image displays the final end results of the ingestion.
#### Step 2: Data Profiling
* This step explain the data profiling in the AWS environment.
* Now using Data profiling we will understand the structure of the data so as we can maintain the quality standards.
* For this we first need to understand the various columns and their data-type we will be using in the analysis.
![image 004](https://github.com/user-attachments/assets/e3b1c55d-1b5d-4e5a-9aaa-6eba2ed07432)<br>
* We can also understand the information of the data stored in the dataset like the meta information, the data types of each column data and other information related to dataset here.
![image 005](https://github.com/user-attachments/assets/bc7f812f-37db-45a1-b0c7-2bf192f3d4a3)<br>
* For this we run the above displayed ‘Data Profiling’ job available after creating the dataset “Vancouver-park-dataset-abdul” in AWS Glue DataBrew.
![image 006](https://github.com/user-attachments/assets/0287d559-8bf5-4d27-938e-0e4f5436ac0e)<br>
* The above indicates the results generated by the data profiling job.
* From above we can see that there are some cells which have missing values.
* We will now move on to clean and structure the data for further usage.
#### Step 3: Data Cleaning 
* This step explaing the Data cleaning done using the AWS DataBrew service.
* To start with the cleaning first he created a project called “vancouver-park-dataset-cleaningproject-abdul” in ‘AWS Glue DataBrew.
![image 007-1](https://github.com/user-attachments/assets/a4a963d3-252d-4aef-beed-4a6718217ff0)<br>
* The above images displays all the cleaning job details we are going to implement.
![image 008](https://github.com/user-attachments/assets/c24f8fc9-f26a-4321-8b01-683eaed37f92)<br>
* We can see the job created by implementing all the changes mentioned in the above cleaning process for each column respectively.
![image 009-1](https://github.com/user-attachments/assets/00cb9136-3658-4024-b602-8694ac4d18aa)<br>
* The end results after running the cleaning job are stored in the ‘tmp’ folder under the ‘Data-Cleaning’ folder inside the ‘vprp-tfm-abdul’ bucket inside S3. The results are displayed as above.
#### Step 4: Data Pipeline Design 
* This step explain the process of designing a ETL pipeline to transform raw data into structured data.
* We can now move to the final stage of creating the ETL pipeline for the dataset selected to do the exploratory analysis of our part. For this I will use AWS Glue service. I created the ETL named “vprp-parks-official-pipeline-abdul” in AWS Glue.
From below pipeline information you can see that first I loaded the information of parks into the console I then dropped unwanted columns and only selected those I need. I then segregated into two one where I filtered the parks for those that are official and then fond count of those in each neighbourhood. For the other part I just found the total count of parks in each neighbourhood.
![image 015](https://github.com/user-attachments/assets/6210f18d-7b4b-4b3e-9796-7da771c46aa3)<br>
* From above pipeline information you can see that first I loaded the information of parks into the console I then dropped unwanted columns and only selected those I need.
* I then segregated into two one where I filtered the parks that are official and then found count of those in each neighbourhood.
* For the other part I just found the total count of parks in each neighbourhood.
* Next I joined these two segregations by merging using the ‘Join’ function.
* I then used a new function of derived column to find the percentage ofof official parks in each neighbourhood and then stored the details using the ‘official’ and ‘neighbourhood’ partitions.
![image 016](https://github.com/user-attachments/assets/e9e75267-d85f-4683-80ef-a210b44baa32)<br>
* Above image shows the output of the pipeline in generated to calculate the percentage of official parks in each neighbourhood.
![image 017](https://github.com/user-attachments/assets/661d3dec-3db1-46dd-b035-8d51c1581e95)<br>
* Above images shows the ETL job run details.
![image 018](https://github.com/user-attachments/assets/98a03522-6246-4b7c-9a48-0c57c600ee2f)<br>
* Above image displays the resulted stored in the ‘neighbourhood’ folder using the partition key.
![image 019](https://github.com/user-attachments/assets/fc0a6a2b-3530-4e97-9126-e287486b8d56)<br>
* Above image displays the resulted stored in the ‘Official’ folder using the partition key.
